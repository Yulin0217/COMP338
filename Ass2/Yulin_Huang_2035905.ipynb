{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## COMP338 ASS2\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fb7efbbf859440"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import package"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7479a1461a70be73"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T02:01:16.281517500Z",
     "start_time": "2023-12-13T02:01:16.262549100Z"
    }
   },
   "id": "91d143573b9e0689"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Cuda Cores and set device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81907b6560b36985"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device using:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device using: ', device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T02:01:16.307219200Z",
     "start_time": "2023-12-13T02:01:16.285561700Z"
    }
   },
   "id": "e9cc493a8c44768a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MiniVGGNet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7dfe9365da5d9ba"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class MiniVGGNet(nn.Module):\n",
    "    def __init__(self, inputShape, classes):\n",
    "        super(MiniVGGNet, self).__init__()\n",
    "\n",
    "        # Initialize the channel dimension, this will be used for batch normalization.\n",
    "        chanDim = 1 if inputShape[0] == 1 else 3\n",
    "\n",
    "        # First set of CONV => RELU => CONV => RELU => POOL layers\n",
    "        # This set of layers has 32 filters and uses 'same' padding to preserve spatial dimensions.\n",
    "        self.conv1a = nn.Conv2d(inputShape[0], 32, (3, 3), padding=\"same\")\n",
    "        self.bn1a = nn.BatchNorm2d(32)\n",
    "        self.conv1b = nn.Conv2d(32, 32, (3, 3), padding=\"same\")\n",
    "        self.bn1b = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)  # Pooling to reduce spatial dimensions\n",
    "        self.dropout1 = nn.Dropout(0.25)  # Dropout for regularization\n",
    "\n",
    "        # Second set of CONV => RELU => CONV => RELU => POOL layers\n",
    "        # Increasing the number of filters to 64 for deeper feature extraction.\n",
    "        self.conv2a = nn.Conv2d(32, 64, (3, 3), padding=\"same\")\n",
    "        self.bn2a = nn.BatchNorm2d(64)\n",
    "        self.conv2b = nn.Conv2d(64, 64, (3, 3), padding=\"same\")\n",
    "        self.bn2b = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)  # Further reducing dimensions\n",
    "        self.dropout2 = nn.Dropout(0.25)  # Additional dropout\n",
    "\n",
    "        # First (and only) set of FC => RELU layers\n",
    "        # The feature map is flattened and fed into fully connected layers.\n",
    "        self.fc1 = nn.Linear(64 * (inputShape[1] // 4) * (inputShape[2] // 4), 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.dropout_fc1 = nn.Dropout(0.5)  # Dropout to prevent overfitting\n",
    "\n",
    "        # Final softmax classifier that outputs probability distributions over the classes.\n",
    "        self.fc2 = nn.Linear(512, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applying the first set of layers followed by activation, batch normalization, and pooling\n",
    "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "        x = F.relu(self.bn1b(self.conv1b(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Applying the second set of layers with the same pattern as above\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Flatten the convolutional layer's output to feed it into the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout_fc1(x)\n",
    "\n",
    "        # Output layer with a softmax to obtain probabilities for each class\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T02:01:16.348321400Z",
     "start_time": "2023-12-13T02:01:16.307219200Z"
    }
   },
   "id": "6159634697bc73e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function Used for Data Loading, Training and Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c98c4b234257ace"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(model_f, data_loader):\n",
    "    model_f.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model_f(data)\n",
    "            loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    loss /= len(data_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def load_data_and_transform(train_batch_size, test_batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train_model(model_f, train_loader_f, test_loader_f, epochs_f):\n",
    "    for epoch in epochs_f:\n",
    "        model_f.train()  # Set the model to training mode\n",
    "        train_loss = 0  # Initialize the loss for this epoch\n",
    "\n",
    "        # Iterate over the training data\n",
    "        with tqdm(train_loader_f, unit=\"batch\") as tepoch:\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "                # Move data to the appropriate device \n",
    "                data, target = data.to(device), target.to(device)\n",
    "                # Zero the gradients before the forward pass\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass: compute the output of the model\n",
    "                output = model_f(data)\n",
    "                # Compute the loss\n",
    "                loss = criterion(output, target)\n",
    "                # Backward pass: compute the gradients of the loss w.r.t. the model's parameters\n",
    "                loss.backward()\n",
    "                # Perform a single optimization step (parameter update)\n",
    "                optimizer.step()\n",
    "                # Accumulate the training loss\n",
    "                train_loss += loss.item()\n",
    "    \n",
    "        # Calculate and record training loss and accuracy\n",
    "        train_loss, train_accuracy = calculate_loss_and_accuracy(model_f, train_loader_f)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        print(f'End of Epoch {epoch + 1}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "    \n",
    "        # Record the training metrics to TensorBoard\n",
    "        writer.add_scalar('Training Loss', train_loss, epoch)\n",
    "        writer.add_scalar('Training Accuracy', train_accuracy, epoch)\n",
    "    \n",
    "        # Calculate and record test loss and accuracy\n",
    "        test_loss, test_accuracy = calculate_loss_and_accuracy(model_f, test_loader_f)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f'End of Epoch {epoch + 1}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "        # Record the test metrics to TensorBoard\n",
    "        writer.add_scalar('Test Loss', test_loss, epoch)\n",
    "        writer.add_scalar('Test Accuracy', test_accuracy, epoch)\n",
    "\n",
    "def test_model(model_f, test_loader_f):\n",
    "    model_f.eval()  # Switch model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for data, target in tqdm(test_loader_f, desc=\"Test\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model_f(data)\n",
    "    \n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader_f.dataset)\n",
    "    # Print test results\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader_f.dataset)} ({100. * correct / len(test_loader_f.dataset):.2f}%)')\n",
    "\n",
    "\n",
    "def polt_for_train_test(epochs_f, train_losses_f, train_accuracies_f, test_losses_f, test_accuracies_f):\n",
    "    # Plot Training and Test Accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_f, train_accuracies_f, 'bo-', label='Train Accuracy')\n",
    "    plt.plot(epochs_f, test_accuracies_f, 'ro-', label='Test Accuracy')\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)  # Ensure y-axis starts at 0 and ends at 1\n",
    "    plt.legend()  # Include a legend to differentiate between train and test lines\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Training and Test Loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_f, train_losses_f, 'bo-', label='Train Loss')\n",
    "    plt.plot(epochs_f, test_losses_f, 'ro-', label='Test Loss')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, 1)  # Ensure y-axis starts at 0 and ends at 1\n",
    "    plt.legend()  # Include a legend for clarity\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T02:01:16.350322800Z",
     "start_time": "2023-12-13T02:01:16.325293700Z"
    }
   },
   "id": "70ca5f99e50afa60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiate the model and define the loss function and optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d95408501ba7ce94"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_shape = (1, 28, 28)  # The image dimensions for the Fashion MNIST dataset\n",
    "num_classes = 10  # The number of classes in the Fashion MNIST dataset\n",
    "model = MiniVGGNet(input_shape, num_classes).to(device)  # Move the model to the appropriate device (CPU or GPU)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss is commonly used for classification tasks\n",
    "optimizer = Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T02:01:16.380415Z",
     "start_time": "2023-12-13T02:01:16.333294900Z"
    }
   },
   "id": "d29aa15d0cf07bcb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4061335eb89251a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 118/118 [00:06<00:00, 18.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 1, Training Loss: 0.2711, Training Accuracy: 90.13%\n",
      "End of Epoch 1, Test Loss: 0.3026, Test Accuracy: 88.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  93%|█████████▎| 110/118 [00:06<00:00, 17.40batch/s]"
     ]
    }
   ],
   "source": [
    "# Start a writer to store train log\n",
    "writer = SummaryWriter('Ass2/runs/fashion_mnist_experiment')\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_accuracies = []  # List to store training accuracy per epoch\n",
    "test_accuracies = []  # List to store test accuracy per epoch\n",
    "train_losses = []  # List to store training loss per epoch\n",
    "test_losses = []  # List to store test loss per epoch\n",
    "\n",
    "# Set epochs\n",
    "epochs = range(0, 2)\n",
    "#Load data and set batch size\n",
    "train_loader, test_loader = load_data_and_transform(train_batch_size=512, test_batch_size=1000)\n",
    "train_model(model,train_loader,test_loader,epochs)\n",
    "\n",
    "# Create a dummy input tensor that matches the model's input dimensions\n",
    "dummy_input = torch.zeros(1, *input_shape).to(device)  \n",
    "\n",
    "# Add the model graph to TensorBoard\n",
    "writer.add_graph(model, dummy_input)\n",
    "\n",
    "# Close the SummaryWriter\n",
    "writer.close()\n",
    "\n",
    "# Save the model to a file\n",
    "torch.save(model, 'Ass2/model/CNN2_model.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-13T02:01:16.349321500Z"
    }
   },
   "id": "bb708af02a4080fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Test the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8265726b816448f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load model from local\n",
    "# model = torch.load('Ass2/model/CNN2_model.pth')\n",
    "\n",
    "# Evaluate the model\n",
    "test_model(model,test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e9d95bf6558e1afd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the accuracy and loss for training and testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c74602f1964c5261"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "polt_for_train_test(epochs, train_losses, train_accuracies, test_losses, test_accuracies)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d051aa93b9c6070a"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cf1f81545e58f8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name}: {param}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c2cd67b799fea65f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ml",
   "language": "python",
   "display_name": "ML"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
