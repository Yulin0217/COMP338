{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# COMP338 ASS2\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fb7efbbf859440"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Yulin Huang 201676465"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4664d0094be88dca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import package"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7479a1461a70be73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91d143573b9e0689"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Cuda Cores availability and set device if cuda is available"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81907b6560b36985"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device using: ', device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9cc493a8c44768a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MiniVGGNet with Dropout "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7dfe9365da5d9ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use resource from when build the function: \n",
    "https://pyimagesearch.com/2019/02/11/fashion-mnist-with-keras-and-deep-learning/\n",
    "                                           https://github.com/khanhnamle1994/fashion-mnist/blob/master/CNN-4Conv.ipynb\n",
    "                                           https://github.com/matvi/miniVGGNet\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb449b46ebd5e40f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MiniVGGNet(nn.Module):\n",
    "    def __init__(self, inputShape, classes):\n",
    "        super(MiniVGGNet, self).__init__()\n",
    "\n",
    "        # First set of CONV => RELU => CONV => RELU => POOL layers\n",
    "        # This set of layers has 32 filters and uses 'same' padding to preserve spatial dimensions.\n",
    "        self.conv1a = nn.Conv2d(inputShape[0], 32, (3, 3), padding=\"same\")\n",
    "        self.bn1a = nn.BatchNorm2d(32)\n",
    "        self.conv1b = nn.Conv2d(32, 32, (3, 3), padding=\"same\")\n",
    "        self.bn1b = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)  # Pooling to reduce spatial dimensions\n",
    "        self.dropout1 = nn.Dropout(0.25)  # Dropout for regularization\n",
    "\n",
    "        # Second set of CONV => RELU => CONV => RELU => POOL layers\n",
    "        # Increasing the number of filters to 64 for deeper feature extraction.\n",
    "        self.conv2a = nn.Conv2d(32, 64, (3, 3), padding=\"same\")\n",
    "        self.bn2a = nn.BatchNorm2d(64)\n",
    "        self.conv2b = nn.Conv2d(64, 64, (3, 3), padding=\"same\")\n",
    "        self.bn2b = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)  # Further reducing dimensions\n",
    "        self.dropout2 = nn.Dropout(0.25)  # Additional dropout\n",
    "\n",
    "        # First (and only) set of FC => RELU layers\n",
    "        # The feature map is flattened and fed into fully connected layers.\n",
    "        self.fc1 = nn.Linear(64 * (inputShape[1] // 4) * (inputShape[2] // 4), 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.dropout_fc1 = nn.Dropout(0.5)  # Dropout to prevent overfitting\n",
    "\n",
    "        # Final softmax classifier that outputs probability distributions over the classes.\n",
    "        self.fc2 = nn.Linear(512, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applying the first set of layers followed by activation, batch normalization, and pooling\n",
    "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "        x = F.relu(self.bn1b(self.conv1b(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Applying the second set of layers with the same pattern as above\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Flatten the convolutional layer's output to feed it into the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout_fc1(x)\n",
    "\n",
    "        # Output layer with a softmax to obtain probabilities for each class\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6159634697bc73e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MiniVGGNet without Dropout "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69a5058f82f77fa4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# class MiniVGGNet(nn.Module):\n",
    "#     def __init__(self, inputShape, classes):\n",
    "#         super(MiniVGGNet, self).__init__()\n",
    "# \n",
    "#         # First set of CONV => RELU => CONV => RELU => POOL layers\n",
    "#         self.conv1a = nn.Conv2d(inputShape[0], 32, (3, 3), padding=\"same\")\n",
    "#         self.bn1a = nn.BatchNorm2d(32)\n",
    "#         self.conv1b = nn.Conv2d(32, 32, (3, 3), padding=\"same\")\n",
    "#         self.bn1b = nn.BatchNorm2d(32)\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2)  # Pooling to reduce spatial dimensions\n",
    "# \n",
    "#         # Second set of CONV => RELU => CONV => RELU => POOL layers\n",
    "#         self.conv2a = nn.Conv2d(32, 64, (3, 3), padding=\"same\")\n",
    "#         self.bn2a = nn.BatchNorm2d(64)\n",
    "#         self.conv2b = nn.Conv2d(64, 64, (3, 3), padding=\"same\")\n",
    "#         self.bn2b = nn.BatchNorm2d(64)\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2)  # Further reducing dimensions\n",
    "# \n",
    "#         # Fully connected layers\n",
    "#         self.fc1 = nn.Linear(64 * (inputShape[1] // 4) * (inputShape[2] // 4), 512)\n",
    "#         self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "# \n",
    "#         # Final softmax classifier\n",
    "#         self.fc2 = nn.Linear(512, classes)\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         # Applying the first set of layers followed by activation, batch normalization, and pooling\n",
    "#         x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "#         x = F.relu(self.bn1b(self.conv1b(x)))\n",
    "#         x = self.pool1(x)\n",
    "# \n",
    "#         # Applying the second set of layers\n",
    "#         x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "#         x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "#         x = self.pool2(x)\n",
    "# \n",
    "#         # Flatten the output and apply the fully connected layers\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = F.relu(self.bn_fc1(self.fc1(x)))\n",
    "# \n",
    "#         # Output layer with a softmax\n",
    "#         x = self.fc2(x)\n",
    "#         return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a158b1b9981d8b76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions for Data Loading, Training, Testing, and Plotting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c98c4b234257ace"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data_and_transform(train_batch_size, test_batch_size):\n",
    "    # Define a transformation pipeline to preprocess the images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert to PyTorch tensor format\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize the tensor with mean 0.5 and standard deviation 0.5\n",
    "    ])\n",
    "\n",
    "    # Load the FashionMNIST training dataset with transformations\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    # Create a DataLoader \n",
    "    train_loader_f = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    # Load the FashionMNIST test dataset with transformations\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    # Create a DataLoader for the test dataset \n",
    "    test_loader_f = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader_f, test_loader_f\n",
    "\n",
    "\n",
    "# This function is for calculating the accuracy and loss on TEST SET when TRAINING the model\n",
    "def calculate_loss_and_accuracy(model_f, data_loader):\n",
    "    # Set model to evaluation mode and initialize metrics\n",
    "    model_f.eval()\n",
    "    loss, correct, total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():  # No gradient computation for evaluation\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model_f(data)\n",
    "            loss += loss_function(output, target).item()  # Accumulate loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get predicted class\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
    "            total += target.size(0)\n",
    "    # Calculate average loss and accuracy\n",
    "    loss /= len(data_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(model_f, train_loader_f, test_loader_f, epochs_f):\n",
    "    for epoch in epochs_f:\n",
    "        model_f.train()  # Set the model to training mode\n",
    "        train_loss = 0  # Initialize the loss for this epoch\n",
    "\n",
    "        # Iterate over the training data\n",
    "        with tqdm(train_loader_f, unit=\"batch\") as tqdm_epoch:\n",
    "            for data, target in tqdm_epoch:\n",
    "                tqdm_epoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "                # Move data to the appropriate device \n",
    "                data, target = data.to(device), target.to(device)\n",
    "                # Zero the gradients before the forward pass\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass: compute the output of the model\n",
    "                output = model_f(data)\n",
    "                # Compute the loss\n",
    "                loss = loss_function(output, target)\n",
    "                # Backward pass: compute the gradients of the loss\n",
    "                loss.backward()\n",
    "                # Perform a single optimization step (parameter update)\n",
    "                optimizer.step()\n",
    "                # Accumulate the training loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "        # Calculate and record training loss and accuracy\n",
    "        train_loss, train_accuracy = calculate_loss_and_accuracy(model_f, train_loader_f)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        print(f'End of Epoch {epoch + 1}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "        # Record the training metrics to TensorBoard\n",
    "        writer.add_scalar('Training Loss', train_loss, epoch)\n",
    "        writer.add_scalar('Training Accuracy', train_accuracy, epoch)\n",
    "\n",
    "        # Calculate and record test loss and accuracy\n",
    "        test_loss, test_accuracy = calculate_loss_and_accuracy(model_f, test_loader_f)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f'End of Epoch {epoch + 1}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "        # Record the test metrics to TensorBoard\n",
    "        writer.add_scalar('Test Loss', test_loss, epoch)\n",
    "        writer.add_scalar('Test Accuracy', test_accuracy, epoch)\n",
    "    return model_f\n",
    "\n",
    "\n",
    "def test_and_plot_incorrect(model_f, test_loader_f, num_classes, num_images=9):\n",
    "    model_f.eval()  # Switch model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Evaluate model without gradient updates\n",
    "    with torch.no_grad():\n",
    "        # Iterate over test dataset\n",
    "        for data, target in tqdm(test_loader_f, desc=\"Test\"):\n",
    "            # Move data and labels to specified device (GPU/CPU)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Calculate model output\n",
    "            output = model_f(data)\n",
    "            # Calculate loss\n",
    "            test_loss += loss_function(output, target).item()\n",
    "            # Determine predicted class with highest probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # Count correct predictions in a batch\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            # Store predictions\n",
    "            all_predictions.extend(pred.view_as(target).cpu().numpy())\n",
    "            # Store actual targets\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # Calculate average loss over test dataset\n",
    "    test_loss /= len(test_loader_f.dataset)\n",
    "\n",
    "    # Print test results\n",
    "    print(\n",
    "        f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader_f.dataset)} ({100. * correct / len(test_loader_f.dataset):.2f}%)')\n",
    "\n",
    "    # Generate and print the classification report\n",
    "    target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "    print(classification_report(all_targets, all_predictions, target_names=target_names))\n",
    "\n",
    "    # Identify incorrect predictions\n",
    "    incorrect_indices = np.nonzero(np.array(all_predictions) != np.array(all_targets))[0]\n",
    "\n",
    "    # Plot incorrect predictions\n",
    "    if len(incorrect_indices) > 0:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for i in range(min(num_images, len(incorrect_indices))):\n",
    "            idx = incorrect_indices[i]\n",
    "            plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(test_loader_f.dataset[idx][0].squeeze(), cmap='gray', interpolation='none')\n",
    "            plt.title(f\"Predicted: {all_predictions[idx]}, True: {all_targets[idx]}\")\n",
    "            plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No incorrect predictions to display.\")\n",
    "\n",
    "\n",
    "def polt_for_train_test(epochs_f, train_losses_f, train_accuracies_f, test_losses_f, test_accuracies_f):\n",
    "    # Plot Training and Test Accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_f, train_accuracies_f, 'bo-', label='Train Accuracy')\n",
    "    plt.plot(epochs_f, test_accuracies_f, 'ro-', label='Test Accuracy')\n",
    "    plt.title('Training and Test Accuracy(%)')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 100)  # Ensure y-axis starts at 0 and ends at 100\n",
    "    plt.legend()  # Include a legend to differentiate between train and test lines\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Training and Test Loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_f, train_losses_f, 'bo-', label='Train Loss')\n",
    "    plt.plot(epochs_f, test_losses_f, 'ro-', label='Test Loss')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, 5)  # Ensure y-axis starts at 0 and ends at 5\n",
    "    plt.legend()  # Include a legend for clarity\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70ca5f99e50afa60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiate the model and define the loss function and optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d95408501ba7ce94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_shape = (1, 28, 28)  # The image dimensions for the Fashion MNIST dataset\n",
    "num_classes = 10  # The number of classes in the Fashion MNIST dataset\n",
    "model = MiniVGGNet(input_shape, num_classes).to(device)  # Move the model to  device (CPU or CUDA)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()  # Using CrossEntropyLoss as Loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d29aa15d0cf07bcb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4061335eb89251a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Start a writer to store train log\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment')\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_accuracies = []  # List to store training accuracy per epoch\n",
    "test_accuracies = []  # List to store test accuracy per epoch\n",
    "train_losses = []  # List to store training loss per epoch\n",
    "test_losses = []  # List to store test loss per epoch\n",
    "\n",
    "# Set epochs\n",
    "epochs = range(0, 50)\n",
    "#Load data and set batch size\n",
    "# ****Noticeï¼šI'm currently using RTX4090 for the training, \n",
    "# don't need to worry about graphic memory, so a bigger batch_size would help accelerate training (4 seconds per epoch).\n",
    "# May need to change the batch size when using COLAB or CPU\n",
    "train_loader, test_loader = load_data_and_transform(train_batch_size=1024, test_batch_size=32)\n",
    "model = train_model(model, train_loader, test_loader, epochs)\n",
    "\n",
    "# Create a dummy input tensor that matches the model's input dimensions\n",
    "dummy_input = torch.zeros(1, *input_shape).to(device)\n",
    "\n",
    "# Add the model graph to TensorBoard\n",
    "writer.add_graph(model, dummy_input)\n",
    "\n",
    "# Close the SummaryWriter\n",
    "writer.close()\n",
    "\n",
    "# Save the model to a file\n",
    "torch.save(model, 'model/MiniVGGNet.pth')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb708af02a4080fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Test the model and plot incorrect prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8265726b816448f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use resource from when build the function: https://github.com/khanhnamle1994/fashion-mnist/blob/master/CNN-4Conv.ipynb"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "579ec4df307a28d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load model from local\n",
    "# model = torch.load('model/MiniVGGNet.pth')\n",
    "\n",
    "# The correct labels of this data set\n",
    "label_descriptions = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "}\n",
    "\n",
    "# Printing the label \n",
    "print(\"The true label of the data set are: \")\n",
    "for label, description in label_descriptions.items():\n",
    "    print(f\"Label: {label}, {description}\")\n",
    "\n",
    "# Evaluate the model\n",
    "test_and_plot_incorrect(model, test_loader, 10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9d95bf6558e1afd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the accuracy and loss for training and testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c74602f1964c5261"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "polt_for_train_test(epochs, train_losses, train_accuracies, test_losses, test_accuracies)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d051aa93b9c6070a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf1f81545e58f8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary(model, input_shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2cd67b799fea65f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ml",
   "language": "python",
   "display_name": "ML"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
