{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## COMP338 ASS2\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fb7efbbf859440"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import package"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7479a1461a70be73"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T01:19:01.418705Z",
     "start_time": "2023-12-13T01:19:01.408613600Z"
    }
   },
   "id": "91d143573b9e0689"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Cuda Cores and set device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81907b6560b36985"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device using:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device using: ', device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T01:19:02.546542100Z",
     "start_time": "2023-12-13T01:19:02.534233600Z"
    }
   },
   "id": "e9cc493a8c44768a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MiniVGGNet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7dfe9365da5d9ba"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class MiniVGGNet(nn.Module):\n",
    "    def __init__(self, inputShape, classes):\n",
    "        super(MiniVGGNet, self).__init__()\n",
    "\n",
    "        # Initialize the channel dimension, this will be used for batch normalization.\n",
    "        chanDim = 1 if inputShape[0] == 1 else 3\n",
    "\n",
    "        # First set of CONV => RELU => CONV => RELU => POOL layers\n",
    "        # This set of layers has 32 filters and uses 'same' padding to preserve spatial dimensions.\n",
    "        self.conv1a = nn.Conv2d(inputShape[0], 32, (3, 3), padding=\"same\")\n",
    "        self.bn1a = nn.BatchNorm2d(32)\n",
    "        self.conv1b = nn.Conv2d(32, 32, (3, 3), padding=\"same\")\n",
    "        self.bn1b = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)  # Pooling to reduce spatial dimensions\n",
    "        self.dropout1 = nn.Dropout(0.25)  # Dropout for regularization\n",
    "\n",
    "        # Second set of CONV => RELU => CONV => RELU => POOL layers\n",
    "        # Increasing the number of filters to 64 for deeper feature extraction.\n",
    "        self.conv2a = nn.Conv2d(32, 64, (3, 3), padding=\"same\")\n",
    "        self.bn2a = nn.BatchNorm2d(64)\n",
    "        self.conv2b = nn.Conv2d(64, 64, (3, 3), padding=\"same\")\n",
    "        self.bn2b = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)  # Further reducing dimensions\n",
    "        self.dropout2 = nn.Dropout(0.25)  # Additional dropout\n",
    "\n",
    "        # First (and only) set of FC => RELU layers\n",
    "        # The feature map is flattened and fed into fully connected layers.\n",
    "        self.fc1 = nn.Linear(64 * (inputShape[1] // 4) * (inputShape[2] // 4), 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.dropout_fc1 = nn.Dropout(0.5)  # Dropout to prevent overfitting\n",
    "\n",
    "        # Final softmax classifier that outputs probability distributions over the classes.\n",
    "        self.fc2 = nn.Linear(512, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applying the first set of layers followed by activation, batch normalization, and pooling\n",
    "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "        x = F.relu(self.bn1b(self.conv1b(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Applying the second set of layers with the same pattern as above\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Flatten the convolutional layer's output to feed it into the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout_fc1(x)\n",
    "\n",
    "        # Output layer with a softmax to obtain probabilities for each class\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T01:19:03.568853700Z",
     "start_time": "2023-12-13T01:19:03.559838Z"
    }
   },
   "id": "6159634697bc73e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function Used for Data Loading, Training and Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c98c4b234257ace"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(model_f, data_loader):\n",
    "    model_f.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model_f(data)\n",
    "            loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    loss /= len(data_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def load_data_and_transform(train_batch_size, test_batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train_model(model_f, train_loader_f, test_loader_f, epochs_f):\n",
    "    for epoch in epochs_f:\n",
    "        model_f.train()  # Set the model to training mode\n",
    "        train_loss = 0  # Initialize the loss for this epoch\n",
    "\n",
    "        # Iterate over the training data\n",
    "        with tqdm(train_loader_f, unit=\"batch\") as tepoch:\n",
    "            for data, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "                # Move data to the appropriate device \n",
    "                data, target = data.to(device), target.to(device)\n",
    "                # Zero the gradients before the forward pass\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass: compute the output of the model\n",
    "                output = model_f(data)\n",
    "                # Compute the loss\n",
    "                loss = criterion(output, target)\n",
    "                # Backward pass: compute the gradients of the loss w.r.t. the model's parameters\n",
    "                loss.backward()\n",
    "                # Perform a single optimization step (parameter update)\n",
    "                optimizer.step()\n",
    "                # Accumulate the training loss\n",
    "                train_loss += loss.item()\n",
    "    \n",
    "        # Calculate and record training loss and accuracy\n",
    "        train_loss, train_accuracy = calculate_loss_and_accuracy(model_f, train_loader_f, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        print(f'End of Epoch {epoch + 1}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "    \n",
    "        # Record the training metrics to TensorBoard\n",
    "        writer.add_scalar('Training Loss', train_loss, epoch)\n",
    "        writer.add_scalar('Training Accuracy', train_accuracy, epoch)\n",
    "    \n",
    "        # Calculate and record test loss and accuracy\n",
    "        test_loss, test_accuracy = calculate_loss_and_accuracy(model_f, test_loader_f, criterion)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f'End of Epoch {epoch + 1}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "        # Record the test metrics to TensorBoard\n",
    "        writer.add_scalar('Test Loss', test_loss, epoch)\n",
    "        writer.add_scalar('Test Accuracy', test_accuracy, epoch)\n",
    "\n",
    "def test_model(model_f, test_loader_f):\n",
    "    model_f.eval()  # Switch model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for data, target in tqdm(test_loader_f, desc=\"Test\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model_f(data)\n",
    "    \n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader_f.dataset)\n",
    "    # Print test results\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader_f.dataset)} ({100. * correct / len(test_loader_f.dataset):.2f}%)')\n",
    "\n",
    "\n",
    "def polt_for_train_test(epochs_f, train_losses_f, train_accuracies_f, test_losses_f, test_accuracies_f):\n",
    "    # Plot Training and Test Accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_f, train_accuracies_f, 'bo-', label='Train Accuracy')\n",
    "    plt.plot(epochs_f, test_accuracies_f, 'ro-', label='Test Accuracy')\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)  # Ensure y-axis starts at 0 and ends at 1\n",
    "    plt.legend()  # Include a legend to differentiate between train and test lines\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Training and Test Loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_f, train_losses_f, 'bo-', label='Train Loss')\n",
    "    plt.plot(epochs_f, test_losses_f, 'ro-', label='Test Loss')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, 1)  # Ensure y-axis starts at 0 and ends at 1\n",
    "    plt.legend()  # Include a legend for clarity\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T01:22:20.390072300Z",
     "start_time": "2023-12-13T01:22:20.385066300Z"
    }
   },
   "id": "70ca5f99e50afa60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiate the model and define the loss function and optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d95408501ba7ce94"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_shape = (1, 28, 28)  # The image dimensions for the Fashion MNIST dataset\n",
    "num_classes = 10  # The number of classes in the Fashion MNIST dataset\n",
    "model = MiniVGGNet(input_shape, num_classes).to(device)  # Move the model to the appropriate device (CPU or GPU)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss is commonly used for classification tasks\n",
    "optimizer = Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T01:19:24.896494900Z",
     "start_time": "2023-12-13T01:19:24.866928900Z"
    }
   },
   "id": "d29aa15d0cf07bcb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4061335eb89251a"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  27%|██▋       | 32/118 [00:01<00:04, 18.37batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m#Load data and set batch size\u001B[39;00m\n\u001B[0;32m     13\u001B[0m train_loader, test_loader \u001B[38;5;241m=\u001B[39m load_data_and_transform(train_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m, test_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m)\n\u001B[1;32m---> 14\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Create a dummy input tensor that matches the model's input dimensions\u001B[39;00m\n\u001B[0;32m     17\u001B[0m dummy_input \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39minput_shape)\u001B[38;5;241m.\u001B[39mto(device)  \n",
      "Cell \u001B[1;32mIn[23], line 54\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model_f, train_loader_f, test_loader_f, epochs_f)\u001B[0m\n\u001B[0;32m     52\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     53\u001B[0m \u001B[38;5;66;03m# Perform a single optimization step (parameter update)\u001B[39;00m\n\u001B[1;32m---> 54\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# Accumulate the training loss\u001B[39;00m\n\u001B[0;32m     56\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    368\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    369\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    370\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    371\u001B[0m             )\n\u001B[1;32m--> 373\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    374\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    376\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[1;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\optim\\adam.py:163\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    152\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    155\u001B[0m         group,\n\u001B[0;32m    156\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    160\u001B[0m         max_exp_avg_sqs,\n\u001B[0;32m    161\u001B[0m         state_steps)\n\u001B[1;32m--> 163\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    170\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\optim\\adam.py:311\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    309\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 311\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    317\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    319\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    321\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    322\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    323\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    324\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    325\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    327\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\optim\\adam.py:565\u001B[0m, in \u001B[0;36m_multi_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[0;32m    563\u001B[0m     exp_avg_sq_sqrt \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001B[0;32m    564\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 565\u001B[0m     exp_avg_sq_sqrt \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_foreach_sqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_exp_avg_sqs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    567\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001B[0;32m    568\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Start a writer to store train log\n",
    "writer = SummaryWriter('Ass2/runs/fashion_mnist_experiment')\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_accuracies = []  # List to store training accuracy per epoch\n",
    "test_accuracies = []  # List to store test accuracy per epoch\n",
    "train_losses = []  # List to store training loss per epoch\n",
    "test_losses = []  # List to store test loss per epoch\n",
    "\n",
    "# Set epochs\n",
    "epochs = range(0, 20)\n",
    "#Load data and set batch size\n",
    "train_loader, test_loader = load_data_and_transform(train_batch_size=512, test_batch_size=1000)\n",
    "train_model(model,train_loader,test_loader,epochs)\n",
    "\n",
    "# Create a dummy input tensor that matches the model's input dimensions\n",
    "dummy_input = torch.zeros(1, *input_shape).to(device)  \n",
    "\n",
    "# Add the model graph to TensorBoard\n",
    "writer.add_graph(model, dummy_input)\n",
    "\n",
    "# Close the SummaryWriter\n",
    "writer.close()\n",
    "\n",
    "# Save the model to a file\n",
    "torch.save(model, 'Ass2/model/CNN2_model.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T01:22:37.724208100Z",
     "start_time": "2023-12-13T01:22:35.733723900Z"
    }
   },
   "id": "bb708af02a4080fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Test the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8265726b816448f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Load model from local\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# model = torch.load('Ass2/model/CNN2_model.pth')\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[43mtest_model\u001B[49m(model,test_loader)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load model from local\n",
    "# model = torch.load('Ass2/model/CNN2_model.pth')\n",
    "\n",
    "# Evaluate the model\n",
    "test_model(model,test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T01:18:31.197753100Z",
     "start_time": "2023-12-13T01:18:30.562685600Z"
    }
   },
   "id": "e9d95bf6558e1afd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the accuracy and loss for training and testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c74602f1964c5261"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "polt_for_train_test(epochs, train_losses, train_accuracies, test_losses, test_accuracies)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-13T01:18:31.199752800Z"
    }
   },
   "id": "d051aa93b9c6070a"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cf1f81545e58f8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name}: {param}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T01:18:31.205260400Z",
     "start_time": "2023-12-13T01:18:31.200753400Z"
    }
   },
   "id": "c2cd67b799fea65f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ml",
   "language": "python",
   "display_name": "ML"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
